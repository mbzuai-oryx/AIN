
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIN: The Arabic Inclusive Multimodal Model</title>
    <link rel="icon" type="image/png" href="images/AIN.png">
    <style>
        .table-container {
            max-width: 60%;  /* Matches the fig-container width */
            margin: 30px auto;
            overflow-x: auto;
        }
        .performance-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9em;
        }
        .performance-table th, 
        .performance-table td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: center;
        }
        .performance-table th {
            background-color: #e0e0e0;
            font-weight: bold;
        }
        .performance-table tr:nth-child(even) {
            background-color: #f8f8f8;
        }
        .performance-table td.total-column {
            background-color: #d0d0d0;
        }
        .performance-table tr:hover {
            background-color: #f5f5f5;
        }
        .table-caption {
            width: 100%;
            text-align: center;
            margin-top: 10px;
            font-style: italic;
            color: #666;
            font-size: 0.9em;
        }  
        .badge-button {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            padding: 0.5rem 1rem;
            background-color: #7a7a7a;
            color: white;
            border-radius: 0.5rem;
            text-decoration: none;
            margin-right: 1rem;
            transition: background-color 0.2s;
        }
        .badge-button:hover {
            background-color: #3d9399;
        }
        .badge-button img {
            width: 1.5rem;
            height: 1.5rem;
        }
        body {
            position: relative;
        }
        .demo-links {
            text-align: center;
            margin: 10px 0;
            background-color: #e6e6e6;
            padding: 20px;
            border-radius: 10px;
        }
        .demo-links a {
            color: #333;
            margin: 0 20px;
            text-decoration: none;
            display: inline-flex;
            align-items: center;
        }
        .demo-links img {
            width: 40px;
            height: 40px;
            margin-right: 8px;
        }
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color:  white;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: white;
            box-shadow:  white;
            border-radius: 8px;
        }
        .container1 {
            max-width: 1200px;
            margin: 0 auto;
            padding: 10px;
            background-color: linear-gradient(to right, #b8fbe9, #c2f9fe);
            border-radius: 10px;
        }
        .header {
            text-align: center;
            margin-bottom: 30px;
        }
        .logo {
            width:22%;
            display: block;
            margin: -5px auto;
        }
        .container {
            text-align: center;
        }
        h1, h2 {
            text-align: center;
            margin: 30px 0;
        }
        .fig-container img {
            max-width: 65%; /* Reduced from 80% */
            height: auto;
            margin: 0 auto;
            display: block;
        }
        .fig-container-side {
            text-align: center;
            margin: 0px 0;
        }
        .fig-container-side img:first-child {
            width: 50%;
            margin: 0 2%;
            display: inline-block;
        }
        .fig-container-side img:last-child {
            width: 40%;
            margin: 0 2%;
            display: inline-block;
        }
        p {
            text-align: center;
            max-width: 800px;
            margin: 20px auto;
        }
        ul {
            display: inline-block;
            text-align: left;
            margin: 0 auto;
        }
        .features {
            text-align: center;
        }
        .features ul {
            padding-left: 20px;
        }
        .badges {
            text-align: center;
        }
        .publication-authors {
            text-align: center;
            max-width: 800px;
            margin: 20px auto;
        }
        .citation {
            text-align: left;
            max-width: 600px;
            margin: 20px auto;
            padding: 20px;
        }
        .title {
            font-size: 6.5em;
            margin: 0px 0;
            background: linear-gradient(to right, #333333, #666666);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: bold;
            font-family:Geneva, Tahoma, sans-serif
        }
        .title2 {
            font-size:2.5em;
            margin: 0px 0;
            background: linear-gradient(to right, #585858, #666666);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-family:Geneva, Tahoma, sans-serif
        }
        .authors {
           font-size: 18px;
            margin: 20px 0;
        }
        .badges {
            margin: 20px 0;
        }
        .badges img {
            margin: 0 5px;
            max-width: 120%;
        }
        .fig-container {
            text-align: center;
            margin: 30px 0;
        }
        .fig-container img {
            max-width: 60%;
            height: auto;
        }
        .fig-container-s {
            text-align: center;
            margin: 30px 0;
        }
        .fig-container-s img {
            max-width: 40%;
        }
        .caption {
            font-style: italic;
            color: #666;
            margin-top: 10px;
            font-size: 0.9em;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: center;
        }
        th {
            background-color: #e0e0e0;
        }
        .logos {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px;
            margin-top: 40px;
        }
        .logos img {
            height: 50px;
        }
        a {
            color: #007bff;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .features {
            margin: 30px 0;
        }
        .section {
            margin: 20px 0;
        }
        .citation {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 4px;
            font-family: monospace;
        }
        .abstract-container {
            max-width: 900px;
            margin: 20px auto;
            padding: 20px 20px;
            background-color:white;
        }
        .abstract-container h2 {
            text-align: center;
            margin-bottom: 20px;
            color:  #333;
        }
        .abstract-content {
            text-align: justify;
            line-height: 1.6;
            margin: 0;
            color: #2c2c2c;
        }
        .abstract-content p {
            margin-bottom: 15px;
        }
        .abstract-content p:last-child {
            margin-bottom: 0;
        }
    </style>
</head>
<body>
           <img src="images/AIN_logo_git_5.png" alt="AIN Logo" class="logo">
    <!--  <img src="images/AIN.png" alt="AIN Logo" class="logo"> <h1 class="title">AIN</h1>-->
            <h1 class="title2">The Arabic INclusive Multimodal Model</h1>
            <!-- Authors -->
            <div class="authors">
                <p>
                    Ahmed Heakl<sup style="color:#3399FF;">1*</sup>, 
                    Sara Ghaboura<sup style="color:#3399FF;">1*</sup>, 
                    Omkar Thawakar<sup style="color:#3399FF;">1</sup>, <br>
                    Fahad S. Khan<sup style="color:#3399FF;">1, </sup><sup style="color:#FF66B3;">2</sup>, 
                    Hisham Cholakkal<sup style="color:#3399FF;">1</sup>,
                    Rao M. Anwer<sup style="color:#3399FF;">1, </sup><sup style="color:#4CB5AE;">3</sup>,
                    Salman Khan<sup style="color:#3399FF;">1, </sup><sup style="color:#FFA280;">4</sup>
                </p>
                <p>
                    <sup style="color:#3399FF;">1</sup>Mohamed bin Zayed University of AI, 
                    <sup style="color:#4CB5AE;">3</sup>Aalto University, 
                    <sup style="color:#FF66B3;">2</sup>Link√∂ping University, 
                    <sup style="color:#FFA280;">4</sup>Australian National University
                </p>
            </div>

            <div class="badges">
                <a href="https://arxiv.org/abs/2502.00094" class="badge-button">
                  <img src="images/pdf.png" alt="Paper"> Paper
                </a>
                <a href="https://arxiv.org/abs/2502.00094" class="badge-button">
                  <img src="images/arxiv.png" alt="AIN"> arxiv
                </a>
                <a href="https://github.com/mbzuai-oryx/AIN" class="badge-button">
                  <img src="images/github.png" alt="Code"> Github
                </a>
              </div>

    
        <div class="container1">
            <div class="demo-links">
                <a href="https://huggingface.co/spaces/ahmedheakl/AIN-Arabic-VLM" target="_blank">
                    <img src="https://cdn-icons-png.flaticon.com/512/1698/1698535.png" alt="Demo">
                    AIN Chatbot
                </a>
                <a href="https://t.me/arabicvlm_bot" target="_blank">
                    <img src="https://cdn-icons-png.flaticon.com/512/2111/2111646.png" alt="Telegram">
                    AIN Telegram
                </a>
                <a href="https://wa.me/46738645096" target="_blank">
                    <img src="https://cdn-icons-png.flaticon.com/512/3670/3670051.png" alt="WhatsApp">
                    AIN WhatsApp
                </a>
            </div>
        </div>



        <!-- AIN Can See Figure -->
        <div class="fig-container">
            <img src="images/ain_can_see.png" alt="AIN Can See">
            <p class="caption">
                AIN: A versatile LMM excelling in visual and contextual understanding across diverse domains, including VQA on complex topics, OCR for various fonts and handwriting, cultural insights (traditions, food, places), agricultural tasks (crop identification, fruit classification, disease detection), remote sensing (multi-scale objects), medical imaging (various modalities), and video analysis (animation, human activities)
            </p>
        </div>

        <!-- Abstract -->
            <div class="abstract-container">
                <h2>Abstract</h2>
                <div class="abstract-content">
                    <p>
                        Amid the swift progress of large language models (LLMs) and their evolution into large multimodal models (LMMs), significant strides have been made in high-resource languages such as English and Chinese. While Arabic LLMs have seen notable progress, Arabic LMMs remain largely unexplored, often narrowly focusing on a few specific aspects of the language and visual understanding. To bridge this gap, we introduce <b><em>AIN - the Arabic Inclusive Multimodal Model-</em></b> designed to excel across diverse domains.
                        AIN is an English-Arabic <b>bilingual LMM</b> designed to excel in English and Arabic, leveraging carefully constructed <b>3.6 million</b> high-quality Arabic-English multimodal data samples. AIN demonstrates state-of-the-art Arabic performance, while also possessing strong English-language visual capabilities.
                    </p>
                </div>
            </div>


           <div align="center">
        <video width="620" height="360" controls>
          <source src="images/AIN_teaser_2.mp4" type="video/mp4">
        </video>
        </div>

        <!-- Latest Updates -->
        <div class="section">
            <h2>üì¢ Latest Updates</h2>
            <p>üî• <b>Jan 2025</b> AIN-7B model, the first Arabic Inclusive LMM, is released ü§ó<br>
            üöÄ Model weights will be released soon.</p>
        </div>

        <!-- Key Features -->
        <div class="features">
            <h1>üåü Key Features</h1>
            <ul>
                <li>First Arabic-centric inclusive Large Multimodal Model (LMM) trained on 3.6M samples</li>
                <li>Includes 35% authentic Arabic data within its Arabic data subset</li>
                <li>Achieves superior performance compared to both open- and closed-source models</li>
                <li>Demonstrates robust bilingual capabilities (Arabic/English)</li>
                <li>Exhibits advanced cultural understanding and domain expertise</li>
            </ul>
        </div>

        <!-- Performance Analysis Figures -->
        <div class="fig-container-s">
            <img src="images/radar_chart.png" alt="Performance Analysis">
            <p class="caption"><strong> Figure 1:</strong>  The chart showcases a comprehensive performance analysis of AIN-7B across CAMEL-Bench domains, comparing it with prominent closed-source models and open-source counterparts.<em><b>OCR:</b> "OCR & Document Understanding", <b>Video:</b> "General Video & Multi-Image Understanding", <b>RS:</b> "Remote Sensing Understanding", <b>CDT:</b> "Chart, Diagram & Table Understanding", <b>Agro.:</b> "Agricultural Image Understanding", <b>Cultural:</b> "Cultural-Specific Understanding", <b>Medical:</b> "Medical Image Understanding".</em></p>
        </div>

        <div class="fig-container">
            <img src="images/intro_bar.png" alt="Comparative Performance">
            <p class="caption"> <strong>Figure 2:</strong> Comparative performance of AIN-7B against other models across key domains, including OCR & Document Understanding, Remote Sensing, Agricultural Understanding, and overall performance across all domains.</p>
        </div>


<div class="table-container">
    <table class="performance-table">
        <thread>
        <tr style="background-color: #e0e0e0;">
            <th>Models</th>
            <th>VQA</th>
            <th>OCR</th>
            <th>Video</th>
            <th>RS</th>
            <th>CDT</th>
            <th>Agro.</th>
            <th>Cult.</th>
            <th>Med.</th>
            <th style="background-color: #d0d0d0;">Total</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>GPT-4o</td>
            <td>ü•à55.15</td>
            <td>ü•à54.98</td>
            <td>ü•á69.65</td>
            <td>ü•à27.36</td>
            <td>ü•à62.35</td>
            <td>ü•à80.75</td>
            <td>ü•á80.86</td>
            <td>ü•á49.91</td>
            <td style="background-color: #d0d0d0;">ü•à60.13</td>
        </tr> 
        <tr>
            <td>GPT-4o-mini</td>
            <td>48.83</td>
            <td>39.38</td>
           <td>ü•à 66.28</td>
            <td>16.93</td>
            <td>56.37</td>
            <td>78.80</td>
            <td>65.92</td>
           <td>ü•à 47.37</td>
            <td style="background-color: #d0d0d0;">52.49</td>
        </tr>
        <tr>
            <td>Gemini-1.5-Pro</td>
            <td>46.68</td>
            <td>28.68</td>
            <td>42.95</td>
            <td>17.07</td>
            <td>47.06</td>
            <td>72.14</td>
            <td>56.24</td>
            <td>33.78</td>
            <td style="background-color: #d0d0d0;">52.38</td>
        </tr>
        <tr>
            <td>Gemini-1.5-flash</td>
            <td>45.59</td>
            <td>27.58</td>
            <td>53.31</td>
            <td>14.95</td>
            <td>48.26</td>
            <td>76.07</td>
            <td>46.54</td>
            <td>42.87</td>
            <td style="background-color: #d0d0d0;">44.40</td>
        </tr>
        <tr>
            <td>InternVL-8B </td>
            <td>30.41 </td>
            <td>15.91 </td>
            <td>51.42 </td>
            <td>5.36 </td>
            <td>30.27 </td>
            <td>44.47 </td>
            <td>20.88 </td>
            <td>29.48 </td>
            <td style="background-color: #d0d0d0;">28.52 </td>
        </tr>
        <tr>
            <td>InternVL2.5-1B </td>
            <td>27.22 </td>
            <td>19.45 </td>
            <td>38.20 </td>
            <td>3.39 </td>
            <td>30.75 </td>
            <td>39.53 </td>
            <td>35.68 </td>
            <td>21.27 </td>
            <td style="background-color: #d0d0d0;">26.94 </td>
        </tr>
        <tr>
            <td>Qwen-VL-2B </td>
            <td>41.02 </td>
            <td>22.93 </td>
            <td>38.90 </td>
            <td>12.56 </td>
            <td>27.83 </td>
            <td>52.02 </td>
            <td>34.28 </td>
            <td>29.12 </td>
            <td style="background-color: #d0d0d0;">32.33 </td>
        </tr>
        <tr>
            <td>AIN-7B <em>(ours)</em> </td>
           <td>ü•á56.78 </td>
            <td>ü•á72.35 </td>
            <td>64.09 </td>
            <td>ü•á45.92 </td>
           <td>ü•á64.10 </td>
            <td>ü•á85.05 </td>
           <td>ü•à78.09 </td>
            <td>43.77 </td>
            <td style="background-color: #d0d0d0;">üèÜ63.77 </td>
        </tr>
    </tbody>
    </table>
    <p class="table-caption">
        <strong>Table 1.</strong>  Performance comparison of AIN and different closed- and open-source LMMs across CAMEL-Bench domains.<em>Best performance is marked with ü•á; second-best is ü•à.</em>
    </p>
</div>
        <!-- Qualitative Analysis -->
        <div class="fig-container-s fig-container-side">
         <!--   <img src="images/contre.png" alt="Qualitative Comparison"> -->
            <img src="images/qualitative.png" alt="Qualitative Examples">
            <p class="caption"><strong>Figure 3:</strong>  Qualitative examples showcasing AIN-7B‚Äôs capabilities across various domains, including general VQA, OCR & Document Understanding, Remote Sensing, Medical Imaging, Agricultural Understanding, and Cultural-Specific tasks.</p>
        </div>        

        <!-- Verification Pipeline -->
        <div class="section">
            <h2>üßê Data Verification and Toxicity Filtering</h2>
            <div class="fig-container">
                <img src="images/verify_pipeline.png" alt="Verification Pipeline">
                <p class="caption"><strong>Figure 4:</strong>  Data verification and filtering pipeline for textual and visual data, ensuring high-quality training data through semantic similarity checks, translation quality evaluations, and toxicity screening for safety compliance.</p>
            </div>
        </div>


        <!-- Citation -->
        <div class="section">
            <h2>üìö Citation</h2>
            <div class="citation">
                @misc{heakl2025ainarabicinclusivelarge, <br>   
                &nbsp;&nbsp;title={AIN: The Arabic INclusive Large Multimodal Model}, <br>
                &nbsp;&nbsp;author={Ahmed Heakl and Sara Ghaboura and Omkar Thawkar and Fahad Shahbaz Khan and Hisham Cholakkal and Rao Muhammad Anwer and Salman Khan},<br>
                &nbsp;&nbsp;year={2025},<br>
                &nbsp;&nbsp;url={https://arxiv.org/abs/2502.00094},<br>
                }
            </div>
        </div>
    

        <!-- Logos -->
        <div class="logos">
            <img src="images/IVAL_logo.png" alt="IVAL Logo">
            <img src="images/Oryx_logo.png" alt="Oryx Logo">
            <img src="images/MBZUAI_logo.png" alt="MBZUAI Logo">
        </div>
    </div>
</body>
</html>
